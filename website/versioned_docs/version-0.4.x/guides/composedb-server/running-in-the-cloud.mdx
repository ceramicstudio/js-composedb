# Running in the Cloud
Run a ComposeDB server in the cloud

## Things to Know
- ComposeDB Server requires running a Ceramic node (which uses IPFS) for decentralized data, [IPFS](https://ipfs.tech/), and a Postgres DB. Each should be within a separate Docker container.
- This guide is focused on running in the cloud using docker. If you use npm, you can follow the same steps as [Running Locally](../../guides/composedb-server/running-locally.mdx).
- Docker images for IPFS are built from the [`go-ipfs-daemon`](https://github.com/ceramicnetwork/go-ipfs-daemon) repository and come pre-configured with plugins that make it easy to run IPFS on cloud infrastructure (e.g. the [S3 plugin](https://github.com/ipfs/go-ds-s3)). Images built from the `main` branch are tagged with `latest`, and the git commit hash of the code from which the image was built.
- Docker images to run ComposeDB Server are built from the [js-ceramic](https://github.com/ceramicnetwork/js-ceramic) repository. Images built from the `main` branch are tagged with `latest`, the git commit hash of the code from which the image was built, and the npm package version of the corresponding [`@ceramicnetwork/cli`](https://www.npmjs.com/package/@ceramicnetwork/cli) release.

:::danger

To run a Ceramic node in production, it is critical to persist the [Ceramic state store](https://developers.ceramic.network/run/nodes/nodes/#ceramic-state-store) and the [IPFS datastore](https://github.com/ipfs/go-ipfs/blob/master/docs/config.md#datastorespec). The form of storage you choose should also be configured for disaster recovery with data redundancy, and some form of snapshotting and/or backups. **Loss of this data can result in permanent loss of Ceramic streams and will cause your node to be in a corrupt state.**

:::

## Cloud Requirements
**Supported Operating Systems**

- Linux
- Mac
- Windows

:::note

For Windows, Windows Subsystem for Linux 2 (WSL2) is strongly recommended. Using the Windows command line is not portable and can cause compatibility issue when running the same configuration on a different operating system (e.g. in a Linux-based cloud deployment).

:::

**Compute**

You’ll need sufficient compute to power Ceramic, IPFS, and Postgres. At a minimum:

- 4 vCPUs
- 8GB RAM

## Kubernetes on the Cloud

You can run ComposeDB Server on Kubernetes on the cloud, such as [Google Kubernetes Engine](https://cloud.google.com/kubernetes-engine) or [Amazon Elastic Kubernetes Service](https://aws.amazon.com/eks/).
You can also run ComposeDB Server on [DigitalOcean Kubernetes](https://www.digitalocean.com/products/kubernetes/).

Running Kubernetes on the Cloud means a provider will manage the underlying infrastructure for you. You can also run Kubernetes on your own infrastructure, but that is outside the scope of this guide.

### Kubernetes on DigitalOcean

Digital Ocean Kubernetes deployment will require 2 tools, `[kubectl](https://kubernetes.io/docs/tasks/tools)` and `[doctl](https://docs.digitalocean.com/reference/doctl/how-to/install/)`.

With the tools installed, you can proceed to Step 1, `Creating a Kubernetes Cluster`.

#### Creating a Kubernetes Cluster
Log into the [DigitalOcean console](https://cloud.digitalocean.com/login) and click `Create` > `Kubernetes`.

Scroll down to `Choose cluster capacity`. Save yourself some money and choose the smallest cluster size. You can always scale up later. A single node is sufficent for testing this example.

Scroll down to the botton and click `Create Cluster`.

This will take you to a `Getting Started with Kubernetes` page and Digital Ocean will start provisioing your nodes. The process may take up to 10 minutes.

#### `Connecting to Kubernetes`.

We'll use the `doctl` command to authenticate to Digital Ocean and retrieve the credentials for our cluster.

The `Connecting to Kubernetes` step will provide the command to setup cluster authentication.

It will look something like this:

```bash
doctl kubernetes cluster kubeconfig save 362dda8b-b555-4c47-9bf0-1a81cf58e0a8
```

Copy and paste the command into your terminal and press enter.

Click `Continue` to proceed to the next step.

#### Verify cluster connectivity

Digital Ocean provides some command to view the cluster and ensure you're connected successfully.

None of this is required, but it's a good way to verify your cluster is up and running.

#### Deploy ComposeDB Server

Clone the [ceramic-infra-scripts repo](https://github.com/3box/ceramic-infra-scripts.git) and enter the created directory.

```
git clone https://github.com/3box/ceramic-infra-scripts.git
cd ceramic-infra-scripts
```

Deploy the stack!
```
# Create a namespace for the deployment
kubectl create namespace ceramic

# Create the necessary secrets
./k8s/base/composedb/create-secrets.sh

# Apply the deployment
kubectl apply -k k8s/base/composedb/
```

It will take a few minutes for the deployment to pull the docker images and start the containers.

You can watch the process with the following command:

```bash
kubectl get pods --watch --namespace ceramic
```

Use `^c` to exit the watch.

Your deployment should look something like this:

```bash
NAME          READY   STATUS    RESTARTS   AGE
composedb-0   0/1     Running   0          77s
ipfs-0        1/1     Running   0          77s
postgres-0    1/1     Running   0          77s
```

:tada:

#### Check the Deployment logs

You can tail the logs of any of the containers.

For example, watch the ceramic node logs with the following command:

```bash
kubectl logs --follow --namespace ceramic composedb-0
```

#### Access the ComposeDB api

To access the ceramic node, use local port forwarding to access the node from your local machine.

This is a block operation, so you'll need to open a new terminal window to run the next command.

The port forward will stop when the command is exited.

```bash
kubectl port-forward --namespace ceramic composedb-0 7007:7007
Forwarding from 127.0.0.1:7007 -> 7007
Forwarding from [::1]:7007 -> 7007
```

```
# In a new terminal window
$ curl http://127.0.0.1:7007/api/v0/node/healthcheck
Alive!
```

#### Expose the node endpoint to the internet

This simple example will expose the ceramic node to the internet using a Digital Ocean Load Balancer.

```bash
kubectl apply -f k8s/base/composedb/do-lb.yaml
```

You can get the EXTERNAL IP address of the load balancer with the following command:

```bash
kubectl get svc --namespace ceramic composedb-lb
NAME           TYPE           CLUSTER-IP      EXTERNAL-IP       PORT(S)          AGE
composedb-lb   LoadBalancer   10.245.10.130   174.138.109.159   7007:31284/TCP   4m4s
```

Test it out!
```
$ curl http://174.138.109.159:7007/api/v0/node/healthcheck
Alive!
```

#### Where is my data stored?

Each part of the stack (js-ceramic, ipfs, postgres) has its own [Persistent Volume](https://kubernetes.io/docs/concepts/storage/persistent-volumes/).

You can view the volumes with the following command:

```bash
kubectl get PersistentVolumeClaim --namespace ceramic
```

This output includes identifiers for the volume on the cloud provider as well as the size and storage class, which defines the properties of the volume.

#### What is my admin DID and how do I use it to connect?

The ceramic node is configured with an admin DID. This DID is used to authenticate with the ceramic node.
The DID is derived from a seed, which is stored in a kubernetes secret named `ceramic-admin` and the `private-key` key's value is the base64 encoded seed.

While the example deployment creates a random seed for the admin DID, you can use your own seed by creating a secret with the same name and key instead of using the `create-secrets.sh` script.

Example:
```bash
$ kubectl create secret generic ceramic-admin --from-literal=private-key=<YOUR SEED>
```

To view the currently configured admin DID seed, you can use the following command (requires [jq](https://stedolan.github.io/jq/)):

```bash
kubectl get secrets --namespace ceramic ceramic-admin -o json | jq -r '.data."private-key"' | base64 -d
```


#### How do I connect to the Postgres database?

You can create a session to the postgres database with the following command:

```bash
kubectl exec --namespace ceramic -ti postgres-0 -- psql -U ceramic
```

A `postgres` service is also created and can be exposed locally with port-forwarding.

```bash
kubectl port-forward --namespace ceramic svc/postgres 5432
```

The `ceramic` user password randomly generated during deployment.
It is also available in the `postgres-auth` secret.

```bash
$ kubectl get secrets postgres-auth -o yaml
apiVersion: v1
data:
  password: NzNjNzQ4ZDkxM2Y5NGQ2MmQwOTRiYzQ2YzIzMmM4YzdlYzFhODA2MA==
  username: Y2VyYW1pYw==
kind: Secret
...
```

#### How do I connect to the IPFS node?

#### How shut it all down?

To remove the workload from the cluster, you can delete the namespace.

```bash
kubectl delete namespace ceramic
```


## Docker Hub
You can find the ComposeDB server and IPFS Docker images on [Docker Hub](https://hub.docker.com/u/ceramicnetwork).

## Examples

> Make sure to update the examples by setting your variables.

### Running IPFS

For production deployments you should run your own IPFS process manually and point your Ceramic node at it. This is referred to as running IPFS in "remote" mode in the Ceramic `daemon.config.json` file, versus the pre-configured “bundled” mode used for running locally.

```bash
docker pull ceramicnetwork/go-ipfs-daemon:latest

docker run \
  -p 5001:5001 \ # API port
  -p 8011:8011 \ # Healthcheck port
  -v /path_on_volume_for_ipfs_repo:/data/ipfs \
  --name ipfs \
  go-ipfs-daemon
```

### Running Postgres

```bash
docker pull postgres

docker run -d \
  -e POSTGRES_PASSWORD=mysecretpassword \
  -e PGDATA=/var/lib/postgresql/data/pgdata \
  -v /path_on_volume_for_postgres_data:/var/lib/postgresql/data \
  -p 5432:5432 \
  --name postgres \
  postgres
```

You can also follow the examples from the official Postgres Docker image [documentation](https://hub.docker.com/_/postgres).

### Running Ceramic

```bash
docker pull ceramicnetwork/js-ceramic:latest

docker run -d \
  -p 7007:7007 \
  -v /path_on_volume_for_daemon_config:/root/.ceramic/daemon.config.json \
  -v /path_on_volume_for_ceramic_logs:/root/.ceramic/logs \
  -v /path_on_volume_for_ceramic_statestore:/root/.ceramic/statestore \
  -e NODE_ENV=production \
  -e CERAMIC_INDEXING_DB_URI=postgres://username:password@host:5432/dbname \
  --name ceramic \
  js-ceramic --ipfs-api http://ipfs_ip_address:5001
```

### Editing the `daemon.config.json` file

To have these IPFS and Postgres settings persist in your Ceramic node, edit the `daemon.config.json` file to include IPFS information. The default location is `~/.ceramic/daemon.config.json`. For a full file example, see the [Ceramic](https://developers.ceramic.network/run/nodes/nodes/#example-daemonconfigjson) docs.

```bash
...
    "ipfs": {
        "mode": "remote",
        "host": "http://ipfs_ip_address:5001"
    },
...
```

```bash
...
"indexing": {
    "db": "postgres://username:password@host:5432/dbname",
    "allow-queries-before-historical-sync": true,
    "enable-historical-sync": false
  }
...
```

## Next Steps

- Understand the different ways to [configure your server](../../guides/composedb-server/server-configurations.mdx), including choosing a network
- Use your Admin DID to authenticate your node to gain [access to mainnet](../../guides/composedb-server/access-mainnet.mdx)
